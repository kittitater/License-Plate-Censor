


import pandas as pd
import re
import ast
from collections import Counter
import itertools





df = pd.read_csv('dataset.csv')
df.head()


df.describe().T


df.info()


df.shape


df.isnull().sum()


# Fixing typo
df.columns = df.columns.str.strip().str.lower().str.replace(" ", "_")
df = df.rename(columns={'plot_kyeword': 'plot_keyword', 'generes': 'genres'})





# We are not using run time, year, and path to recommend, we should drop the columns.
df = df.drop('run_time', axis=1)
df = df.drop('year', axis=1)
df = df.drop('path', axis=1)
df = df.drop('overview', axis=1)





# Change rating to numbers
df['rating'] = pd.to_numeric(df['rating'], errors='coerce')


def parse_votes(value):
    if isinstance(value, str):
        value = value.strip().upper().replace('K', '000').replace('M', '000000')
    try:
        return int(float(value))
    except:
        return None

df['votes'] = df['user_rating'].apply(parse_votes)





for col in ['genres', 'plot_keyword', 'top_5_casts']:
    df[col] = df[col].apply(lambda x: ast.literal_eval(x) if pd.notna(x) else [])





high_quality_df = df[(df['rating'] >= 6.0) & (df['votes'] >= 10000)]





# Genres Observation
genre_counter = Counter(itertools.chain.from_iterable(high_quality_df['genres']))
print(genre_counter.most_common())





# One-Hot Encode Genres
from sklearn.preprocessing import MultiLabelBinarizer

mlb = MultiLabelBinarizer()
genre_encoded = mlb.fit_transform(high_quality_df['genres'])

genre_df = pd.DataFrame(genre_encoded, columns=mlb.classes_)

high_quality_df = pd.concat([high_quality_df.reset_index(drop=True), genre_df], axis=1)





# Keyword Observation
def normalize_keywords(keywords):
    keywords = [kw.lower().strip() for kw in keywords if isinstance(kw, str)]
    return list(set(keywords))

high_quality_df['plot_keyword'] = high_quality_df['plot_keyword'].apply(normalize_keywords)

keyword_counter = Counter(itertools.chain.from_iterable(high_quality_df['plot_keyword']))
print(keyword_counter.most_common())





threshold = 3
low_freq_keywords = {kw for kw, count in keyword_counter.items() if count <= threshold}
len(low_freq_keywords)


def remove_low_freq_keywords(keywords):
    return [kw for kw in keywords if kw not in low_freq_keywords]

high_quality_df['plot_keyword'] = high_quality_df['plot_keyword'].apply(remove_low_freq_keywords)





from nltk.stem import WordNetLemmatizer
import nltk
nltk.download('wordnet')
lemm = WordNetLemmatizer()

def lemmatize_keywords(keywords):
    return [lemm.lemmatize(kw) for kw in keywords]

high_quality_df['plot_keyword'] = high_quality_df['plot_keyword'].apply(lemmatize_keywords)


high_quality_df.describe().T


high_quality_df.info()


high_quality_df.isnull().sum()


high_quality_df.to_csv('cleaned_high_quality_dataset.csv', index=False)
